{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dec30b-61ad-4818-96ac-e48b277111d1",
   "metadata": {},
   "source": [
    "# Sonotype Model\n",
    "## Can you check the password using keyboard sounds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175fe254-c41f-4d97-857b-3aebeb46c0c5",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The TrAItor has attemped to break into the model and steal our sensitive information. But, you've intercepted an audio recording of the TrAItor typing their password. Can you decipher the password using only the keystroke sounds?\n",
    "\n",
    "### Objective\n",
    "Analyze the audio recordings to determine the exact password. Use audio analysis and pattern recognition to extract the password from the key press sounds. Submit the correct password to complete the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8993b-afbe-426d-91d7-32c56b17349a",
   "metadata": {},
   "source": [
    "The training code in this notebook is based on the following [GitHub gist](https://gist.github.com/Deopoler/95d961587e64d668e27a399e764a94a5) by [Seonjin Kim](https://github.com/Deopoler).\n",
    "\n",
    "In what follows we train a [HuBERT](https://huggingface.co/docs/transformers/en/model_doc/hubert) model to classify an augmented version of the dataset we created in the previous step. For your convenience, the data was packed into .parquet files and uploaded as a Hugging Face dataset to the Hugging Face Hub: [ðŸ¤—christopher/sonotype](https://huggingface.co/datasets/christopher/sonotype).\n",
    "\n",
    "\n",
    "> HuBERT is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.\n",
    ">\n",
    "> HuBERT was fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded using Wav2Vec2CTCTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6362920a-6c25-4f30-8bdb-62348c079a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRUCIBLE_API_KEY = \"\"\n",
    "CHALLENGE = \"sonotype\"\n",
    "CRUCIBLE_URL = \"https://crucible.dreadnode.io\"\n",
    "CHALLENGE_URL = \"https://sonotype.crucible.dreadnode.io\"\n",
    "ARTIFACT_FILES = ['recordings.tar.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dda5661-87c4-4323-83aa-8b48c258002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2e3e909e-8339-4f60-a7b9-a741b5b1ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset(\"christopher/sonotype\", data_dir=\"data/augmented-keystrokes\", split=\"train\")\n",
    "dset = dset.train_test_split(test_size=0.13, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c325e4a6-e1fa-4653-a58e-c0d40f3184b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 6065\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 907\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dc51f13d-5b17-4bfb-aaf1-09363dd2b84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'm_155.wav',\n",
       "  'array': array([-0.00132515, -0.00076744, -0.00011702, ..., -0.00078037,\n",
       "         -0.00104127, -0.0006526 ]),\n",
       "  'sampling_rate': 22050},\n",
       " 'label': 25}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e9090ed-4c9e-4d05-be56-f97c1e2b0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ntu-spml/distilhubert\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id, do_normalize=True, return_attention_mask=True\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    return inputs\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "max_duration = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b2dec4ca-45ab-4305-9918-089224bffb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8870fdb5d8447482eff1f3772ed6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ebfc5d9bde424cae1b3666bec1bf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/907 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset = dset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "dset_encoded = dset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "58a15c53-4564-49b2-95d1-83f09f5a3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_fn = dset[\"train\"].features[\"label\"].int2str\n",
    "\n",
    "id2label = {\n",
    "    str(i): id2label_fn(i)\n",
    "    for i in range(len(dset_encoded[\"train\"].features[\"label\"].names))\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2ce9e25d-ad26-423e-a618-2e616d545c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enter'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[\"17\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e3e76-cc93-459a-9174-1f48fe1f3a5f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3b8404ba-93b2-4651-9e22-504ca9a8146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at ntu-spml/distilhubert and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/1da05489-3812-4f15-a6e5-c8d3c57df39e/infosec/env/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3311231/3391942307.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4560' max='11400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4560/11400 07:05 < 10:38, 10.70 it/s, Epoch 12/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.697800</td>\n",
       "      <td>3.699639</td>\n",
       "      <td>0.056229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.181400</td>\n",
       "      <td>3.131329</td>\n",
       "      <td>0.181918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.568000</td>\n",
       "      <td>2.476310</td>\n",
       "      <td>0.456450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.834700</td>\n",
       "      <td>1.768614</td>\n",
       "      <td>0.717751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.161500</td>\n",
       "      <td>0.884553</td>\n",
       "      <td>0.940463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.259641</td>\n",
       "      <td>0.974642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.033597</td>\n",
       "      <td>0.996692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.005673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4560, training_loss=1.1897149357617947, metrics={'train_runtime': 425.9118, 'train_samples_per_second': 427.201, 'train_steps_per_second': 26.766, 'total_flos': 7.5137157902752e+16, 'train_loss': 1.1897149357617947, 'epoch': 12.0})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 30\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.2,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=dset_encoded[\"train\"],\n",
    "    eval_dataset=dset_encoded[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42eb7d2-1d14-4f86-8758-c88b99d9e90c",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "We now combine the logits of all password characters across the 5 tries at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c66ca805-dd67-4d8e-b81f-3f654e337bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h4k3r k3ystr0k3s'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(trainer.state.best_model_checkpoint)\n",
    "dset = load_dataset(\"christopher/sonotype\", data_dir=\"data/keystrokes\", split=\"train\")\n",
    "logits = []\n",
    "for data_dir in [\"data/password1\",\n",
    "                 \"data/password2\",\n",
    "                 \"data/password3\",\n",
    "                 \"data/password4\",\n",
    "                 \"data/password5\"\n",
    "                ]:\n",
    "    password = load_dataset(\"christopher/sonotype\", data_dir=data_dir, split=\"train\")\n",
    "    password = password.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "    password_encoded = password.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")\n",
    "    l = []\n",
    "    for row in password_encoded:\n",
    "        input_values = torch.tensor(row[\"input_values\"]).unsqueeze(0)\n",
    "        att = torch.tensor(row[\"attention_mask\"]).unsqueeze(0)\n",
    "        l.append(model(input_values, att).logits.detach())\n",
    "    logits.append(torch.stack(l).squeeze())\n",
    "password_tokens = torch.stack(logits).sum(axis=0)\n",
    "password_tokens = torch.topk(password_tokens, 1).indices.numpy()\n",
    "password_decoded = \"\"\n",
    "for token in password_tokens:\n",
    "    password_decoded += id2label[str(token[0])].replace(\"space\", \" \").replace(\"semic\", \";\").replace(\"enter\", \"\").replace(\"period\", \".\")\n",
    "password_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad9814-21f9-4ca7-b29a-022324d7c3c0",
   "metadata": {},
   "source": [
    "The password appears to be a [leetspeak](https://en.wikipedia.org/wiki/Leet) version of **\"hacker keystrokes\"**. Fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d892118a-f9de-47b5-968c-89538ec33383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def query(input_data):\n",
    "    response = requests.post(\n",
    "        f\"{CHALLENGE_URL}/score\",\n",
    "        headers={\"X-API-Key\": CRUCIBLE_API_KEY},\n",
    "        json={\"data\": input_data},\n",
    "    )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b300f22a-e8c0-4f83-aa7a-6017dd841239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Incorrect. Try again.'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"h4k3r k3ystr0k3s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8813363-d019-46b3-817d-9a505e5b9acd",
   "metadata": {},
   "source": [
    "Surprisingly, and much to our dismay during the competition itself, the submission password seems to erroneously contain an additional character that was not in the audio. It turns out that the endpoint is expecting **`h4ck3r k3ystr0k3s`** instead of the decoded **`h4k3r k3ystr0k3s`**. Submitting the correct password does the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9772bf69-e811-4e82-a313-39fc4dce1572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gAAAAABnSh7gYlC9nKrTE3RiH'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"h4ck3r k3ystr0k3s\")[\"flag\"][:25]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}